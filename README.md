# Zero-One Instruction プロジェクト

大規模言語モデル（LLM）を活用して高品質な学習データ（質問と回答のペア）を自動生成するフレームワークです。このシステムは、シンプルなシードプロンプトから始めて、複数の変換ステップを経て、高品質な指示-応答ペアを生成します。

## 概要

このプロジェクトは、基本的なシードプロンプトから複雑で多様なプロンプトを生成し、それに対応する回答を作成することで、自己教師あり学習（SFT）データを効率的に生成することを目的としています。生成プロセスは品質評価ステップを含み、低品質のプロンプトを排除または修正します。

## 特徴

- **複数段階の変換プロセス**: 基本プロンプトから高品質な質問-回答ペアへと段階的に変換
- **品質フィルタリング**: 品質の低いプロンプトを検出・排除または修正
- **バッチ処理**: 効率的な大量データ生成のためのバッチ処理をサポート
- **マルチGPU対応**: 複数GPUを活用した並列処理により処理速度を向上
- **逐次JSONLファイル出力**: 生成データをJSONL形式で逐次出力し、大規模な生成にも対応

## 必要環境

```
vllm==0.8.3
transformers==4.51.0
datasets==3.5.0
```

## システム構成

プロジェクトは以下のコンポーネントで構成されています：

- `main.py`: メインの処理フローを実行するスクリプト
- `gen_seed.py`: シードプロンプトを生成するモジュール
- `vllm_inf.py`: vLLMによるバッチ推論を行うクラス
- `model_config.yml`: モデル設定ファイル
- `auto_run.py`: 複数GPUでの並列実行を管理するスクリプト

## データ生成フロー

シードプロンプトから質の高い指示-応答ペアを生成するプロセスは、以下のステップで構成されています：

1. **シード生成**: `gen_seed.py`が基本となるシードプロンプトを生成
2. **プロンプト拡張（evol_width）**: シードプロンプトから新しい視点を含むプロンプトを生成
3. **品質判定（evol_judge）**: 生成したプロンプトの品質を評価
4. **プロンプト修正（FIXER）**: 低品質と判断されたプロンプトを修正
5. **複雑化（evol_depth）**: プロンプトに複雑さや深みを追加
6. **標準化（evol_flatten）**: プロンプトを整形し標準化
7. **最終品質判定**: 再度品質を評価
8. **回答生成**: 高品質なプロンプトに対して回答を生成
9. **JSONLファイル出力**: 生成された質問-回答ペアをJSONL形式で保存

## 使用方法

### 基本的な使用法（単一プロセス）

```bash
python main.py --start_idx 0 --end_idx 100 --output_file output.jsonl
```

### 複数GPUでの並列実行

```bash
python auto_run.py --num_prompts 1000 --output_file combined_output.jsonl
```

### コマンドラインオプション（main.py）

- `--start_idx`: 処理を開始するシードプロンプトのインデックス
- `--end_idx`: 処理を終了するシードプロンプトのインデックス
- `--output_file`: 出力JSONLファイルのパス
- `--id_start`: 出力レコードIDの開始番号（デフォルト: 0）
- `--config_path`: モデル設定ファイルのパス（デフォルト: "model_config.yml"）

### コマンドラインオプション（auto_run.py）

- `--num_prompts`: 生成するベースプロンプトの総数
- `--output_file`: 最終出力ファイルのパス
- `--main_script`: 実行するメインスクリプトのパス（デフォルト: "main.py"）
- `--num_gpus`: 使用するGPU数（デフォルト: 利用可能な全GPU）

## モデル設定

`model_config.yml`でモデルの各種パラメータを設定できます：

```yaml
model_name: "OPEA/Mistral-Small-3.1-24B-Instruct-2503-int4-AutoRound-awq-sym"
quantization: "awq"
gpu_memory_utilization: 0.7
tensor_parallel_size: null  # nullにすると自動でGPU枚数に合わせる
batch_size: 32
max_tokens: 2048
max_model_len: 4096
trust_remote_code: True
dtype: "auto"
temperature: 0.9
top_p: 0.9
```

## 出力データ形式

生成されるデータはJSONL形式で、各レコードは以下の構造を持ちます：

```json
{
  "id": "record_XXX",
  "input": "生成されたプロンプト",
  "output": "生成された回答",
  "conversation": [
    {"from": "system", "value": "あなたは優秀な日本語AIアシスタントです。ユーザーの質問に対して、正確かつ簡潔な回答を行います。"},
    {"from": "human", "value": "生成されたプロンプト"},
    {"from": "gpt", "value": "生成された回答"}
  ]
}
```

## GPUリソース管理

`auto_run.py`は利用可能なGPUリソースを効率的に管理します：

- GPU数が2の冪（2, 4, 8, 16など）の場合、単一プロセスで処理
- GPU数が2の冪でない場合（例：3, 5, 7）、2の冪の和に分割して並列処理（例：7 = 4+2+1）
- 各GPU群にデータを分割し、プロセスごとに指定されたGPUを割り当てて並列実行
- 結果を自動的に統合して最終出力を生成

## 注意事項

- 大規模なデータ生成を行う場合、十分なディスク容量を確保してください
- モデル読み込みに必要なGPUメモリ量を考慮し、適切なquantizationを設定してください
- 処理性能は、使用するGPUの性能やモデルのサイズによって大きく変わります
